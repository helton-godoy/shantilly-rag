{"question": "O que é o projeto Shantilly RAG?", "answer": "O Shantilly RAG é um repositório dedicado a um pipeline de Retrieval-Augmented Generation que concentra documentação e códigos do ecossistema Charmbracelet (Bubble Tea, Bubbles, Lip Gloss, Gum etc.), projetos derivados e outras bibliotecas Go relacionadas, criando uma base de conhecimento unificada para auxiliar modelos de IA no desenvolvimento.", "tags": ["visao-geral", "shantilly", "rag"]}
{"question": "Quais são as três etapas principais do pipeline do Shantilly RAG?", "answer": "O pipeline do Shantilly RAG é dividido em três etapas: (1) Fetching, que clona/atualiza repositórios e baixa documentos externos; (2) Chunking + Metadados, que converte arquivos brutos em chunks semânticos com metadados; e (3) Indexação no Qdrant, que gera embeddings e faz upsert na coleção configurada.", "tags": ["pipeline", "etapas"]}
{"question": "Qual é o papel do Qdrant no Shantilly RAG?", "answer": "O Qdrant é o banco vetorial usado para armazenar embeddings e metadados dos chunks. Ele permite que o servidor RAG faça buscas vetoriais (e futuramente híbridas) para recuperar os trechos mais relevantes na hora de responder uma pergunta.", "tags": ["qdrant", "infra"]}
{"question": "Qual é o papel do Ollama no Shantilly RAG?", "answer": "O Ollama é utilizado como provedor local de modelos para gerar embeddings (por exemplo usando o modelo nomic-embed-text) e como LLM para gerar respostas no endpoint /query.", "tags": ["ollama", "embeddings", "llm"]}
{"question": "Como o servidor RAG do Shantilly está organizado de forma geral?", "answer": "O servidor RAG fica em server/ e expõe uma API HTTP via FastAPI, com um endpoint /health e um endpoint /query. O /query aplica um pipeline de RAG com reescrita opcional de query, retrieve em Qdrant, reranking e geração de resposta com o LLM, retornando também os documentos usados como contexto.", "tags": ["server", "api", "query"]}
